{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09075257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "import re\n",
    "import wandb\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acb2220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = pd.read_excel(\"train_split.xlsx\")\n",
    "system_prompt = \"\"\"\n",
    "You are a Palestinian historian and geographer specialising in the land, people, and cultural heritage of Palestine.\n",
    "\n",
    "General task\n",
    "Answer questions or carry out instructions accurately and neutrally while preserving the authentic Palestinian narrative.\n",
    "Use Modern Standard Arabic unless the user asks for another language.\n",
    "\n",
    "Narrative-integrity rules\n",
    "Give the Arabic place-name first; add other names in brackets when needed.\n",
    "Avoid terms that erase Palestinian identity (e.g. “Judea and Samaria”) unless requested for comparison.\n",
    "Present the Palestinian account first, then mention other viewpoints briefly if relevant.\n",
    "Replace every standalone occurrence of “Israel” with “the Israeli occupation” or “the occupying entity”.\n",
    "\n",
    "Accuracy rules\n",
    "Admit when you don't know the answer. If the information is not in the provided context or your internal knowledge, state that you cannot provide a factual answer.\n",
    "\n",
    "Output format\n",
    "If asked to “extract a span”, return that text only.\n",
    "Otherwise:\n",
    "• Optional one-line intro, then short bullets or mini paragraphs.\n",
    "Do not exceed 150 words unless the user asks for more.\n",
    "\n",
    "Uncertainty\n",
    "If no trustworthy data exist, state: “No reliable data available as of the last update.”\n",
    "\"\"\"\n",
    "def formatting_prompts_func(batch):\n",
    "    output_texts = []\n",
    "    \n",
    "    for question, answer in zip(batch[\"Question\"], batch[\"Answer\"]):\n",
    "        train_message = (\n",
    "            f\"<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>{system_prompt}<|END_OF_TURN_TOKEN|>\"\n",
    "            f\"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{question}<|END_OF_TURN_TOKEN|>\"\n",
    "            f\"<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>{answer}<|END_OF_TURN_TOKEN|>\"\n",
    "        )\n",
    "        \n",
    "        output_texts.append(train_message)\n",
    "    \n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c8015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"CohereForAI/aya-expanse-8b\"\n",
    "\n",
    "# hyperparamerter configuration\n",
    "QUANTIZE_4BIT = True\n",
    "USE_GRAD_CHECKPOINTING = True\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "TRAIN_MAX_SEQ_LENGTH = 1024\n",
    "USE_FLASH_ATTENTION = True\n",
    "GRAD_ACC_STEPS = 64\n",
    "LORA_R = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "PER_DEVICE_EVAL_BATCH_Size = 4\n",
    "LEARNING_RATE = 4e-4\n",
    "OPTIM = \"paged_adamw_8bit\"\n",
    "GROUP_BY_LENGTH = True\n",
    "LORA_ALPHA = 64\n",
    "WARMUP_RATIO = 0.08\n",
    "WEIGHT_DECAY = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e03b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = pd.read_excel(\"train_split.xlsx\")\n",
    "system_prompt = \"\"\"\n",
    "You are a Palestinian historian and geographer specialising in the land, people, and cultural heritage of Palestine.\n",
    "\n",
    "General task\n",
    "Answer questions or carry out instructions accurately and neutrally while preserving the authentic Palestinian narrative.\n",
    "Use Modern Standard Arabic unless the user asks for another language.\n",
    "\n",
    "Narrative-integrity rules\n",
    "Give the Arabic place-name first; add other names in brackets when needed.\n",
    "Avoid terms that erase Palestinian identity (e.g. “Judea and Samaria”) unless requested for comparison.\n",
    "Present the Palestinian account first, then mention other viewpoints briefly if relevant.\n",
    "Replace every standalone occurrence of “Israel” with “the Israeli occupation” or “the occupying entity”.\n",
    "\n",
    "Accuracy rules\n",
    "Admit when you don't know the answer. If the information is not in the provided context or your internal knowledge, state that you cannot provide a factual answer.\n",
    "\n",
    "Output format\n",
    "If asked to “extract a span”, return that text only.\n",
    "Otherwise:\n",
    "• Optional one-line intro, then short bullets or mini paragraphs.\n",
    "Do not exceed 150 words unless the user asks for more.\n",
    "\n",
    "Uncertainty\n",
    "If no trustworthy data exist, state: “No reliable data available as of the last update.”\n",
    "\"\"\"\n",
    "def formatting_prompts_func(batch):\n",
    "    output_texts = []\n",
    "    \n",
    "    for question, answer in zip(batch[\"Question\"], batch[\"Answer\"]):\n",
    "        train_message = (\n",
    "            f\"<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>{system_prompt}<|END_OF_TURN_TOKEN|>\"\n",
    "            f\"<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{question}<|END_OF_TURN_TOKEN|>\"\n",
    "            f\"<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>{answer}<|END_OF_TURN_TOKEN|>\"\n",
    "        )\n",
    "        \n",
    "        output_texts.append(train_message)\n",
    "    \n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "quantization_config = None\n",
    "if QUANTIZE_4BIT:\n",
    "  quantization_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_quant_type=\"nf4\",\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "  )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          MODEL_NAME,\n",
    "          quantization_config=quantization_config,\n",
    "          torch_dtype=torch.bfloat16,\n",
    "          device_map=\"auto\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5918266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(\n",
    "    dataset,\n",
    "    test_size=0.2,       # 20% for validation\n",
    "    random_state=42,     # for reproducible results\n",
    "    shuffle=True         # Shuffle the data before splitting\n",
    ")\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "eval_dataset = Dataset.from_pandas(val_df, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cc60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=\"results\",\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    gradient_checkpointing=USE_GRAD_CHECKPOINTING,\n",
    "    optim=OPTIM,\n",
    "    save_steps=50,\n",
    "    logging_steps=10,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    report_to=\"none\",\n",
    "    max_seq_length=TRAIN_MAX_SEQ_LENGTH,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    r=LORA_R,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # Ensure model is defined earlier\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,  # Ensure tokenizer is defined earlier\n",
    "    args=training_arguments,\n",
    "    formatting_func=formatting_prompts_func,  # Ensure this function is defined\n",
    "    train_dataset=train_dataset,  # Ensure this is defined or passed to the function\n",
    "    eval_dataset=eval_dataset,  # Ensure this is defined or passed to the function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee90a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "trainer.model.save_pretrained(save_directory=f\"{MODEL_NAME}\")\n",
    "model.config.use_cache = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
