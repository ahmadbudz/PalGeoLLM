{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/quickstart_peft_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PEFT Finetuning Quick Start Notebook\n",
    "\n",
    "This notebook shows how to train a Meta Llama 3 model on a single GPU (e.g. A10 with 24GB) using int8 quantization and LoRA finetuning.\n",
    "\n",
    "**_Note:_** To run this notebook on a machine with less than 24GB VRAM (e.g. T4 with 16GB) the context length of the training dataset needs to be adapted.\n",
    "We do this based on the available VRAM during execution.\n",
    "If you run into OOM issues try to further lower the value of train_config.context_length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Install pre-requirements and convert checkpoint\n",
    "\n",
    "We need to have llama-cookbook and its dependencies installed for this notebook. Additionally, we need to log in with the huggingface_cli and make sure that the account is able to to access the Meta Llama weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if running from Colab T4\n",
    "# ! pip install llama-cookbook ipywidgets\n",
    "\n",
    "# import huggingface_hub\n",
    "# huggingface_hub.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the model\n",
    "\n",
    "Setup training configuration and load the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "import re\n",
    "import wandb\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_name = \"LLaMAX/LLaMAX3-8B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          model_name,\n",
    "          quantization_config=bnb_config,\n",
    "          torch_dtype=torch.bfloat16,\n",
    "          device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Check base model\n",
    "\n",
    "Run the base model on an example input:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the base model only repeats the conversation.\n",
    "\n",
    "### Step 3: Load the preprocessed dataset\n",
    "\n",
    "We load and preprocess the samsum dataset which consists of curated pairs of dialogs and their summarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(row: dict):\n",
    "    system_prompt = \"\"\"\n",
    "    You are a Palestinian historian and geographer specialising in the land, people, and cultural heritage of Palestine.\n",
    "\n",
    "    General task\n",
    "    Answer questions or carry out instructions accurately and neutrally while preserving the authentic Palestinian narrative.\n",
    "    Use Modern Standard Arabic unless the user asks for another language.\n",
    "\n",
    "    Narrative-integrity rules\n",
    "    Give the Arabic place-name first; add other names in brackets when needed.\n",
    "    Avoid terms that erase Palestinian identity (e.g. “Judea and Samaria”) unless requested for comparison.\n",
    "    Present the Palestinian account first, then mention other viewpoints briefly if relevant.\n",
    "    Replace every standalone occurrence of “Israel” with “the Israeli occupation” or “the occupying entity”.\n",
    "\n",
    "    Accuracy rules\n",
    "    Do not state information without a reliable source.\n",
    "\n",
    "    Output format\n",
    "    If asked to “extract a span”, return that text only.\n",
    "    Otherwise:\n",
    "    • Optional one-line intro, then short bullets or mini paragraphs.\n",
    "    • End with “References”.\n",
    "    Do not exceed 150 words unless the user asks for more.\n",
    "\n",
    "    Uncertainty\n",
    "    If no trustworthy data exist, state: “No reliable data available as of the last update.”\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": row[\"Question\"]},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"Answer\"]},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "dataset = pd.read_excel(\"train_split.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df.apply(format_example, axis=1)\n",
    "print(df.text.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(row: dict) -> int:\n",
    "    return len(\n",
    "        tokenizer(\n",
    "            row[\"text\"],\n",
    "            add_special_tokens=True,\n",
    "            return_attention_masks=False,\n",
    "        )[\"input_ids\"]\n",
    "    )\n",
    "df[\"token_count\"] = df.apply(count_tokens, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.token_count, weights=np.ones(len(df.token_count)) / len(df.token_count))\n",
    "plt.gca().yaxis.set_major_formatter(PrecentFormatter(1))\n",
    "plt.xlabel(\"tokens\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Prepare model for PEFT\n",
    "\n",
    "Let's prepare the model for Parameter Efficient Fine Tuning (PEFT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig\n",
    "from dataclasses import asdict\n",
    "# CIDAR configuration values for fine-tuning\n",
    "LORA_R = 32\n",
    "LORA_DROPOUT = 0.1  \n",
    "LORA_ALPHA = 64  \n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    r=LORA_R,\n",
    "    lora_dropout = LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hyperparamerter configuration\n",
    "QUANTIZE_4BIT = True\n",
    "USE_GRAD_CHECKPOINTING = True\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "TRAIN_MAX_SEQ_LENGTH = 1024\n",
    "USE_FLASH_ATTENTION = True\n",
    "GRAD_ACC_STEPS = 64\n",
    "LORA_R = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "PER_DEVICE_EVAL_BATCH_Size = 4\n",
    "LEARNING_RATE = 4e-4\n",
    "OPTIM = \"paged_adamw_8bit\"\n",
    "GROUP_BY_LENGTH = True\n",
    "LORA_ALPHA = 64\n",
    "WARMUP_RATIO = 0.08\n",
    "WEIGHT_DECAY = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(\n",
    "    dataset,\n",
    "    test_size=0.2,       # 20% for validation\n",
    "    random_state=42,     # for reproducible results\n",
    "    shuffle=True         # Shuffle the data before splitting\n",
    ")\n",
    "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "eval_dataset = Dataset.from_pandas(val_df, preserve_index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Fine tune the model\n",
    "\n",
    "Here, we fine tune the model for a single epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=\"results\",\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    gradient_checkpointing=USE_GRAD_CHECKPOINTING,\n",
    "    optim=OPTIM,\n",
    "    save_steps=50,\n",
    "    logging_steps=10,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    report_to=\"none\",\n",
    "    max_seq_length=TRAIN_MAX_SEQ_LENGTH,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    r=LORA_R,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # Ensure model is defined earlier\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,  # Ensure tokenizer is defined earlier\n",
    "    args=training_arguments,\n",
    "    formatting_func=format_example,  # Ensure this function is defined\n",
    "    train_dataset=train_dataset,  # Ensure this is defined or passed to the function\n",
    "    eval_dataset=val_dataset,  # Ensure this is defined or passed to the function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6:\n",
    "Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_directory='LLaMAX3-qlora')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7:\n",
    "Try the fine tuned model on the same example again to see the learning progress:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
