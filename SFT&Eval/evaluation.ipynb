{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fbefca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "from peft import PeftModel # Import PeftModel\n",
    "login(token=\"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\")\n",
    "# Define the base model and the adapter directory\n",
    "\n",
    "# Define the base model and the adapter directory\n",
    "base_model_id = \"CohereForAI/aya-expanse-8b\"  # e.g., \"HuggingFaceH4/zephyr-7b-beta\" or a local path\n",
    "\n",
    "# # Add lora_adapter_path variable\n",
    "# lora_adapter_path = \"LLaMAX3-qlora\" # Replace with the actual path to your adapter\n",
    "\n",
    "# 2. Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "\n",
    "# 3. Load the base model with 4-bit quantization using BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # Changed to load_in_4bit\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # or torch.float16\n",
    "    bnb_4bit_quant_type=\"nf4\", # or \"int8\"\n",
    "    bnb_4bit_use_double_quant=True, # Often recommended for 4-bit\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config, # Use quantization_config\n",
    "    device_map=\"auto\" # Load model across available devices (e.g., GPUs)\n",
    ")\n",
    "\n",
    "# 4. Load the LoRA adapters\n",
    "# model = PeftModel.from_pretrained(model, lora_adapter_path)\n",
    "\n",
    "# 5. (Optional but recommended) Merge LoRA adapters into the base model for evaluation\n",
    "# This step makes the model behave like a regular fine-tuned model without the need for PEFT\n",
    "# during inference/evaluation.\n",
    "# Note: Merging might not be possible or necessary with 4-bit quantization.\n",
    "# model = model.merge_and_unload()\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to generate responses using Hugging Face pipeline\n",
    "def generate_responses(input_file, output_file,\n",
    "                       max_length=1024, temperature=0.2, top_p=1.0, top_k=0,\n",
    "                       repetition_penalty=1.2, do_sample=True, num_return_sequences=1):\n",
    "\n",
    "    print(\"Loading the Hugging Face text-generation pipeline...\")\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, token=token)##rfa\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_name_or_path, token=token)##rfa\n",
    "\n",
    "    # Load the Hugging Face text-generation pipeline\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    print(\"Pipeline loaded successfully.\")\n",
    "\n",
    "    # Read input file (CSV format) using pandas\n",
    "    print(f\"Reading input CSV file: {input_file}\")\n",
    "    # df = pd.read_csv(input_file)\n",
    "    df = pd.read_excel(input_file)\n",
    "    inputs = df['Question'].tolist()  # Assuming the column name is 'instruction'\n",
    "    print(f\"Found {len(inputs)} instructions in the CSV file.\")\n",
    "\n",
    "    # Prepare the output list\n",
    "    outputs = []\n",
    "    print(\"Generating responses...\")\n",
    "\n",
    "    # Loop through each input instruction and generate response\n",
    "    for idx, input_text in enumerate(inputs):\n",
    "        print(f\"Processing instruction {idx + 1}/{len(inputs)}: {input_text[:50]}...\")  # Print first 50 characters for preview\n",
    "\n",
    "        # Craft the prompt to be more concise\n",
    "        # You can experiment with different prompts\n",
    "        prompt = f\"Answer the following question concisely and directly, providing only the requested information without any extra text or conversational filler:\\n\\nQuestion: {input_text.strip()}\\n\\nAnswer:\"\n",
    "\n",
    "        # Generate response using Hugging Face pipeline\n",
    "        generated_responses = generator(prompt, # Use the crafted prompt\n",
    "                                        max_length=max_length, # Use max_length\n",
    "                                        max_new_tokens=1024,\n",
    "                                        temperature=temperature,\n",
    "                                        top_p=top_p,\n",
    "                                        top_k=top_k,\n",
    "                                        repetition_penalty=repetition_penalty,\n",
    "                                        do_sample=do_sample,\n",
    "                                        num_return_sequences=num_return_sequences)\n",
    "\n",
    "        # Collect the generated output (generating multiple responses if needed)\n",
    "        for response in generated_responses:\n",
    "\n",
    "            # Strip the instruction part from the generated text\n",
    "            output_text = response[\"generated_text\"]\n",
    "            # Adjust stripping to remove the crafted prompt\n",
    "            output_without_instruction = output_text[len(prompt):].strip()\n",
    "            print(f\"output_without_instruction: {output_without_instruction}\")\n",
    "            outputs.append({\n",
    "                \"instruction\": input_text.strip(),\n",
    "                \"generated_output\": output_without_instruction ##response[\"generated_text\"]##\n",
    "            })\n",
    "\n",
    "    print(f\"Generated {len(outputs)} responses.\")\n",
    "\n",
    "    # Write the results to a JSON file\n",
    "    print(f\"Saving results to JSON file: {output_file}\")\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(outputs, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"Results saved successfully to {output_file}.\")\n",
    "\n",
    "# Argument parsing for CLI execution\n",
    "def main():\n",
    "    print(\"Starting inference process...\")\n",
    "\n",
    "    # parser = argparse.ArgumentParser(description=\"Generate responses using a language model\")\n",
    "    # parser.add_argument(\"--input_file\", type=str, required=True, help=\"Path to the input CSV file\")\n",
    "    # parser.add_argument(\"--output_file\", type=str, required=True, help=\"Path to the output JSON file\")\n",
    "    # #parser.add_argument(\"--model_name_or_path\", type=str, required=True, help=\"Token for Hugging Face)\")\n",
    "    # #parser.add_argument(\"--token\", type=str, required=True, help=\"Path or name of the model (local or Hugging Face)\")\n",
    "    # parser.add_argument(\"--max_length\", type=int, default=512, help=\"Max length for generated sequences (default: 512)\")\n",
    "    # parser.add_p(type=float, default=1.0, help=\"Top-p for nucleus sampling (default: 1.0)\")\n",
    "    # parser.add_argument(\"--top_k\", type=int, default=0, help=\"Top-k for sampling (default: 0)\")\n",
    "    # parser.add_argument(\"--repetition_penalty\", type=float, default=1.2, help=\"Repetition penalty (default: 1.2)\")\n",
    "    # parser.add_argument(\"--do_sample\", type=bool, default=True, help=\"Whether to sample or not (default: True)\")\n",
    "    # parser.add_argument(\"--num_return_sequences\", type=int, default=1, help=\"Number of sequences to return (default: 1)\")\n",
    "\n",
    "    # # Parse the arguments\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "\n",
    "    # print the DataFrame**\n",
    "    try:\n",
    "        # df_to_print = pd.read_csv(args.input_file)\n",
    "        df_to_print = pd.read_excel(\"DS\\\\test_split.xlsx\") # Updated input file path for printing\n",
    "\n",
    "        print(\"\\n--- Input Dataset (First 5 rows) ---\")\n",
    "        print(df_to_print.head())\n",
    "        print(\"\\n--------------------------------------\\n\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at test_split.xlsx\") # Updated error message\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the input file: {e}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Generate responses based on the provided arguments\n",
    "    generate_responses(\n",
    "        input_file=\"test_split.xlsx\", # Updated input file path\n",
    "        output_file=\"results-aya-gen.json\",\n",
    "        #model_name_or_path=args.model_name_or_path,\n",
    "        #token=args.token,\n",
    "        max_length=1024,\n",
    "        temperature=0.2,\n",
    "        top_p=1,\n",
    "        top_k=0,\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae695e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bert_score\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from evaluate import load\n",
    "import pandas as pd\n",
    "\n",
    "reference_dataset = pd.read_excel(\"test_split.xlsx\")\n",
    "predictions_dataset = pd.read_json(\"aya-results.json\") #the model response\n",
    "references = reference_dataset['Answer'].tolist()\n",
    "predictions = predictions_dataset['generated_output'].tolist()\n",
    "bertscore = load(\"bertscore\")\n",
    "results = bertscore.compute(predictions=predictions, references=references, lang=\"ar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
