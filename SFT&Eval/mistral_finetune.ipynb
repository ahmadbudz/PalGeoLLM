{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26fb83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "import re\n",
    "import wandb\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### conversational format\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361ec2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 20505/20505 [00:01<00:00, 15194.06 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are a helpful assistant designed to answer questions based on a given context passage. Your task is to extract the most accurate answer span directly from the context that best answers the question. Return only the answer text.', 'role': 'system'}, {'content': 'كيف وزعت أراضيها عام 1945؟', 'role': 'user'}, {'content': '539 دونمًا مزروعة ومروية، 2,107 للحبوب، و29 دونمًا مبنية.', 'role': 'assistant'}]\n",
      "Train samples: 16404\n",
      "Test samples: 4101\n"
     ]
    }
   ],
   "source": [
    "#from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "system_prompt = \"\"\"\n",
    "You are a Palestinian historian and geographer specialising in the land, people, and cultural heritage of Palestine.\n",
    "\n",
    "General task\n",
    "Answer questions or carry out instructions accurately and neutrally while preserving the authentic Palestinian narrative.\n",
    "Use Modern Standard Arabic unless the user asks for another language.\n",
    "\n",
    "Narrative-integrity rules\n",
    "Give the Arabic place-name first; add other names in brackets when needed.\n",
    "Avoid terms that erase Palestinian identity (e.g. “Judea and Samaria”) unless requested for comparison.\n",
    "Present the Palestinian account first, then mention other viewpoints briefly if relevant.\n",
    "Replace every standalone occurrence of “Israel” with “the Israeli occupation” or “the occupying entity”.\n",
    "\n",
    "Accuracy rules\n",
    "Admit when you don't know the answer. If the information is not in the provided context or your internal knowledge, state that you cannot provide a factual answer.\n",
    "\n",
    "Output format\n",
    "If asked to “extract a span”, return that text only.\n",
    "Otherwise:\n",
    "• Optional one-line intro, then short bullets or mini paragraphs.\n",
    "Do not exceed 150 words unless the user asks for more.\n",
    "\n",
    "Uncertainty\n",
    "If no trustworthy data exist, state: “No reliable data available as of the last update.”\n",
    "\"\"\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": sample[\"Question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"Answer\"]}\n",
    "    ]\n",
    "  }\n",
    "\n",
    "Dset = pd.read_excel(\"train_split.xlsx\")\n",
    "hf_dataset = Dataset.from_pandas(Dset)\n",
    "# Convert dataset to OAI messages\n",
    "print(hf_dataset[345][\"messages\"])\n",
    "# 4. Split dataset (scaled for 19,900 rows)\n",
    "eval_fraction = 0.2  # 2500/12500 = 20%\n",
    "eval_size = int(len(hf_dataset) * eval_fraction)\n",
    "dataset_split = hf_dataset.train_test_split(test_size=eval_size)\n",
    "\n",
    "# 5. Resulting datasets\n",
    "train_dataset = dataset_split[\"train\"]  # 15,920 samples\n",
    "eval_dataset = dataset_split[\"test\"]    # 3,980 samples\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf05a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\" #24 JUNE 2024\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=True)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    "\n",
    "# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "\n",
    "# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
    "#model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hyperparamerter configuration\n",
    "QUANTIZE_4BIT = True\n",
    "USE_GRAD_CHECKPOINTING = True\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "TRAIN_MAX_SEQ_LENGTH = 1024\n",
    "USE_FLASH_ATTENTION = True\n",
    "GRAD_ACC_STEPS = 64\n",
    "LORA_R = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "NUM_TRAIN_EPOCHS = 1\n",
    "PER_DEVICE_EVAL_BATCH_Size = 4\n",
    "LEARNING_RATE = 4e-4\n",
    "OPTIM = \"paged_adamw_8bit\"\n",
    "GROUP_BY_LENGTH = True\n",
    "LORA_ALPHA = 64\n",
    "WARMUP_RATIO = 0.08\n",
    "WEIGHT_DECAY = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac256ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=\"results\",\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    gradient_checkpointing=USE_GRAD_CHECKPOINTING,\n",
    "    optim=OPTIM,\n",
    "    save_steps=50,\n",
    "    logging_steps=10,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    report_to=\"none\",\n",
    "    max_seq_length=TRAIN_MAX_SEQ_LENGTH,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    r=LORA_R,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # Ensure model is defined earlier\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,  # Ensure tokenizer is defined earlier\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,  # Ensure this is defined or passed to the function\n",
    "    eval_dataset=eval_dataset,  # Ensure this is defined or passed to the function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae7b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ef655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "trainer.model.save_pretrained(save_directory=f\"{model_id}\")\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
